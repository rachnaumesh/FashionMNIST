\section*{Batch Normalization Gradient Derivation}

We will derive the gradient for a batch normalization layer, following the algorithm from the original paper by Sergey Ioffe and Christian Szegedy. We are given a 1-dimensional BN layer with a mini-batch of size $m$, and for every $1 \leq i \leq m$, the gradient $\frac{\partial f}{\partial y_i}$ is given.

\subsection*{a. $\frac{\partial f}{\partial \gamma}$}

Using the chain rule:

\begin{align*}
\frac{\partial f}{\partial \gamma} &= \sum_{i=1}^m \frac{\partial f}{\partial y_i} \cdot \frac{\partial y_i}{\partial \gamma} \\
&= \sum_{i=1}^m \frac{\partial f}{\partial y_i} \cdot \frac{\partial}{\partial \gamma}(\gamma \hat{x}_i + \beta) \\
&= \sum_{i=1}^m \frac{\partial f}{\partial y_i} \cdot \hat{x}_i
\end{align*}

\subsection*{b. $\frac{\partial f}{\partial \beta}$}

Similarly:

\begin{align*}
\frac{\partial f}{\partial \beta} &= \sum_{i=1}^m \frac{\partial f}{\partial y_i} \cdot \frac{\partial y_i}{\partial \beta} \\
&= \sum_{i=1}^m \frac{\partial f}{\partial y_i} \cdot \frac{\partial}{\partial \beta}(\gamma \hat{x}_i + \beta) \\
&= \sum_{i=1}^m \frac{\partial f}{\partial y_i}
\end{align*}

\subsection*{c. $\frac{\partial f}{\partial \hat{x}_i}$}

For a specific $i$:

\begin{align*}
\frac{\partial f}{\partial \hat{x}_i} &= \frac{\partial f}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i} \\
&= \frac{\partial f}{\partial y_i} \cdot \frac{\partial}{\partial \hat{x}_i}(\gamma \hat{x}_i + \beta) \\
&= \frac{\partial f}{\partial y_i} \cdot \gamma
\end{align*}

\subsection*{d. $\frac{\partial f}{\partial \sigma^2}$}

We need to use the chain rule and the fact that $\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$:

\begin{align*}
\frac{\partial f}{\partial \sigma^2} &= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial \sigma^2} \\
&= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{\partial}{\partial \sigma^2}\left(\frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\right) \\
&= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot (x_i - \mu) \cdot (-\frac{1}{2})(\sigma^2 + \epsilon)^{-3/2} \\
&= -\frac{1}{2}\sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot (x_i - \mu) \cdot (\sigma^2 + \epsilon)^{-3/2}
\end{align*}

\subsection*{e. $\frac{\partial f}{\partial \mu}$}

Again, we use the chain rule:

\begin{align*}
\frac{\partial f}{\partial \mu} &= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial \mu} \\
&= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{\partial}{\partial \mu}\left(\frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}\right) \\
&= \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i} \cdot (-\frac{1}{\sqrt{\sigma^2 + \epsilon}}) \\
&= -\frac{1}{\sqrt{\sigma^2 + \epsilon}} \sum_{i=1}^m \frac{\partial f}{\partial \hat{x}_i}
\end{align*}

\subsection*{f. $\frac{\partial f}{\partial x_i}$}

For this final part, we need to consider all paths through which $x_i$ affects $f$:

\begin{align*}
\frac{\partial f}{\partial x_i} &= \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}_i}{\partial x_i} + \frac{\partial f}{\partial \sigma^2} \cdot \frac{\partial \sigma^2}{\partial x_i} + \frac{\partial f}{\partial \mu} \cdot \frac{\partial \mu}{\partial x_i} \\
&= \frac{\partial f}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \\
&\quad (-\frac{1}{2}\sum_{j=1}^m \frac{\partial f}{\partial \hat{x}_j} \cdot (x_j - \mu) \cdot (\sigma^2 + \epsilon)^{-3/2}) \cdot \frac{2(x_i - \mu)}{m} + \\
&\quad (-\frac{1}{\sqrt{\sigma^2 + \epsilon}} \sum_{j=1}^m \frac{\partial f}{\partial \hat{x}_j}) \cdot \frac{1}{m} \\
&= \frac{1}{\sqrt{\sigma^2 + \epsilon}} \left(\frac{\partial f}{\partial \hat{x}_i} - \frac{1}{m} \sum_{j=1}^m \frac{\partial f}{\partial \hat{x}_j}\right) - \\
&\quad \frac{x_i - \mu}{m(\sigma^2 + \epsilon)} \sum_{j=1}^m \frac{\partial f}{\partial \hat{x}_j} \cdot (x_j - \mu)
\end{align*}

These derivations provide the gradients for all components of the batch normalization layer, allowing for efficient backpropagation during training.
